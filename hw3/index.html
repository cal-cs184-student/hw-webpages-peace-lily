<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Abby Brooks-Ramirez, Richard Villagomez</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-peace-lily/hw3/index.html">hw-webpages-peace-lily/hw3/index.html</a><br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-pothos">cal-cs184-student/teams/pothos</a>
		<!--
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure>
-->
		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.
		
		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<p>
			For Part 1, I focused on generating rays from the camera and implementing intersection logic for triangles and spheres. In <code>Camera::generate_ray</code>, I mapped normalized screen coordinates \((x, y)\) to the canonical sensor plane at \(z = -1\) in camera space. Using the horizontal and vertical fields of view, I computed the sensor width and height, centered the plane at the origin, and scaled the input coordinates accordingly. Since the camera looks down the -Z axis, I created rays pointing from the origin through the sensor point, then transformed them into world space using the <code>c2w</code> matrix. Finally, I normalized the direction and initialized the ray with the camera’s near and far clip planes.
			</p>
			
			<p>
			In <code>PathTracer::raytrace_pixel</code>, I generated one or more rays per pixel and averaged the estimated radiance:
			</p>
			
			<ul>
			  <li>If <code>ns_aa == 1</code>, I generated a single ray through the pixel center.</li>
			  <li>If anti-aliasing was enabled, I used <code>gridSampler</code> to generate stratified subpixel offsets and traced <code>ns_aa</code> rays per pixel.</li>
			</ul>
			
			<p>
			I then implemented triangle intersection using the Möller–Trumbore algorithm from Lecture 9 in <code>Triangle::intersect</code>. After computing edge vectors and checking the determinant to avoid degenerate triangles, I solved for barycentric coordinates \((u, v)\) to confirm the intersection is inside the triangle. For valid hits, I used the barycentric weights to interpolate the normal from the triangle’s vertex normals and populated the <code>Intersection</code> struct.
			</p>
			
			<p>
			For spheres, I solved the quadratic ray-sphere intersection equation in <code>Sphere::test</code> and used it in both <code>has_intersection</code> and <code>intersect</code>. I selected the nearest valid root and computed the normal as the normalized vector from the sphere center to the hit point.
			</p>
			
			<p>
			The following debug renderings helped verify everything was working:
			</p>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
					<tr>
						<td colspan="2">
						  <figure>
							<img src="images/banana.png" width="400px"/>
							<figcaption>banana.dae</figcaption>
						  </figure>
						</td>
					  </tr>
				  <tr>
					<td>
					  <img src="images/CBspheres.png" width="400px"/>
					  <figcaption>CBspheres.dae</figcaption>
					</td>
					<td>
					  <img src="images/CBdragon.png" width="400px"/>
					  <figcaption>CBdragon.dae</figcaption>
					</td>
				  </tr>
				</table>
			  </div>
	
	
		<h2>Part 2: Bounding Volume Hierarchy</h2>
			<p>We place our primitives in the BVH recursively. We begin by iterating through the primitives to create a bounding box, which we expand given the bounding box of the primitive. We also create an empty node, set the nodes bounding box to be the box we just calculated and setting the left and right pointers to be null (for not).</p>

			<p>We reach the <b>base case</b> if our primitives are less than or equal to max_leaf_size, at which point we update the pointers and return our new node.</p>
			
			<p>Otherwise, we reach the <b>recursive case </b> and must split our primitives into left and right subgroups to recurse on. We split groups along whichever axis is the longest (i.e. has the biggest difference between max and min points). Initially, we stored the new left and right primitives in new vectors, which we stored on the heap. However, after office hours we were told that a simpler and more accurate method would be to sort the list in place and split it in half (along the longest axis) so we do this instead. Thus our heuristic is to sort the primitives by their centroid longest-axis value, and then split our sorted list in half, so our heuristic effectively becomes the mean centroid of the primitives along the longest axis. We then update the left and right pointers of our node and return them.</p>
			<div style="display: flex; flex-direction: column; align-items: center;">
				<table style="width: 100%; text-align: center; border-collapse: collapse;">
				  <tr>
					<td style="text-align: center;">
					  <img src="images/cow.png" width="400px"/>
					  <figcaption>cow.dae, BVH: 1.58s, No BVH: 2.85s.</figcaption>
					</td>
					<td style="text-align: center;">
					  <img src="images/dragon.png" width="400px"/>
					  <figcaption>dragon.dae, BVH: 1.55s, No BVH: 98.38s.</figcaption>
					</td>
				  </tr>
				  <tr>
					<td style="text-align: center;">
					  <img src="images/max.png" width="400px"/>
					  <figcaption>maxplanck.dae, BVH: 1.60s, No BVH:30.88s.</figcaption>
					</td>
					<td style="text-align: center;">
					  <img src="images/lucy.png" width="400px"/>
					  <figcaption>CBlucy.dae, BVH: 1.29s, No BVH: 134.64s.</figcaption>
					</td>
				  </tr>
				</table>
			</div>

			<p> In the above table, we tested a variety of .dae files, from those with less complex meshes (such as the cow) to those with hundreds of thousands of triangles in their meshes. From the table, it’s clear that using BVH optimizes the rendering time. BVH allows us to broadly check large parts of the mesh to determine whether or not an intersection exists within a certain area if an intersection does not exist, we can skip all the primitives in that bounding box, allowing us to speed up our rendering. It seems that images with more concentrated meshes and detail (especially regarding lighting) in certain areas take longer to render without BVH (i.e. BClucy and dragon takes longer to render than maxplanck and cow). BVH speedup is most significant for the images with more complex geometry (CBlucy and dragon).</p>
		
			<h2>Part 3: Direct Illumination</h2>
<p>
  This part was all about implementing direct lighting using two different sampling strategies: uniform hemisphere sampling and importance sampling. Both approaches try to estimate the light coming from area lights directly, but with different levels of noise and efficiency.
</p>

<p>
  I first implemented <code>estimate_direct_lighting_hemisphere</code>, where I uniformly sampled directions over the hemisphere above the surface. For each sampled direction, I traced a shadow ray from the hit point and checked if it intersected any light-emitting objects. If it did, I applied the reflection equation using the BSDF’s <code>f()</code> function and the cosine between the sampled direction and the surface normal. This worked, but it's very noisy—even with a decently high number of samples—since most directions don't actually point toward light sources.
</p>

<p>
  Then I implemented <code>estimate_direct_lighting_importance</code>. Instead of sampling random directions in the hemisphere, this one directly samples the lights in the scene. For each light, I generated rays toward sampled points on the light and checked if they were occluded. If not, I used the reflectance equation again, normalized by the PDF of the light sample. Point lights were only sampled once since they don’t have area. This method produces way cleaner results with fewer samples.
</p>

<p>
  To test both, I rendered the same scene with hemisphere sampling and then with importance sampling at 1, 4, 16, and 64 light samples. For all of them, I kept the number of camera rays per pixel at 1 to highlight the difference in noise levels.
</p>

<div style="display: flex; flex-direction: column; align-items: center;">
  <table style="width: 100%; text-align: center; border-collapse: collapse;">
    <tr>
      <td>
        <img src="images/CBbunny_H_16_8.png" width="400px"/>
        <figcaption>hemisphere sampling -- 16 light samples, 8 bounces</figcaption>
      </td>
      <td>
        <img src="images/CBbunny_L_1.png" width="400px"/>
        <figcaption>importance sampling -- 1 light sample, 1 pixel sample</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/CBbunny_L_4.png" width="400px"/>
        <figcaption>importance sampling -- 4 light samples, 1 pixel sample</figcaption>
      </td>
      <td>
        <img src="images/CBbunny_L_16.png" width="400px"/>
        <figcaption>importance sampling -- 16 light samples, 1 pixel sample</figcaption>
      </td>
    </tr>
    <tr>
		<td colspan="2" style="text-align: center;">
			<img src="images/CBbunny_L_64.png" width="400px"/>
        <figcaption>importance sampling -- 64 light samples, 1 pixel sample</figcaption>
      </td>
    </tr>
  </table>
</div>

<p>
  Overall, importance sampling was a big improvement. Even at 4 samples, the shadows were a lot cleaner than the hemisphere version with 16. At 64, it’s basically clean with soft shadows and much less noise. The hemisphere sampling "wastes" a lot of samples shooting rays into directions that never hit lights. Importance sampling is more focused and efficient, and makes a noticeable difference overall.
</p>


		<h2>Part 4: Global Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 5: Adaptive Sampling</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul>
		</div>
	</body>
</html>